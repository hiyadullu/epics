{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hiyadullu/epics/blob/main/cloud_notebooks/CREMAD_run.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08f40c45",
      "metadata": {
        "id": "08f40c45",
        "papermill": {
          "duration": 0.044714,
          "end_time": "2024-09-19T03:38:35.861614",
          "exception": false,
          "start_time": "2024-09-19T03:38:35.816900",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# **Emotion Recognition using Multimodal Deep Learning Approaches**\n",
        "\n",
        "In this experiment we propose a novel Multimodal Architecture to predict 8 different human emotions. We use audio and video as inputs to our model. The dataset this notebook runs on is the SAVEE datatset\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnu6bQMMDWQz",
        "outputId": "7d778aba-757a-4948-a14e-2f1ccce65a86"
      },
      "id": "nnu6bQMMDWQz",
      "execution_count": 335,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 336,
      "id": "6bb90469",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-19T03:38:35.939651Z",
          "iopub.status.busy": "2024-09-19T03:38:35.938211Z",
          "iopub.status.idle": "2024-09-19T03:39:52.499101Z",
          "shell.execute_reply": "2024-09-19T03:39:52.497373Z"
        },
        "id": "6bb90469",
        "outputId": "a3af35da-77ff-4fc4-9dfa-0fa3a89bde97",
        "papermill": {
          "duration": 76.605079,
          "end_time": "2024-09-19T03:39:52.502699",
          "exception": false,
          "start_time": "2024-09-19T03:38:35.897620",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: av in /usr/local/lib/python3.12/dist-packages (15.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install av\n",
        "\n",
        "import numpy as np\n",
        "np.float = float  # Fix deprecated np.float usage\n",
        "np.int = int      # Fix deprecated np.int usage\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "try:\n",
        "    from torchsummary import summary\n",
        "except:\n",
        "    !pip install torchsummary\n",
        "    from torchsummary import summary\n",
        "\n",
        "try:\n",
        "    from torcheval.metrics.functional import multiclass_f1_score\n",
        "except:\n",
        "    !pip install torcheval\n",
        "    from torcheval.metrics.functional import multiclass_f1_score\n",
        "\n",
        "try:\n",
        "    import skvideo.io\n",
        "except:\n",
        "    !pip install sk-video\n",
        "    import skvideo.io\n",
        "\n",
        "\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "import os\n",
        "\n",
        "import re\n",
        "\n",
        "\n",
        "\n",
        "import seaborn as sn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "from torchvision.io import read_image, read_video\n",
        "\n",
        "import torchaudio\n",
        "\n",
        "import librosa\n",
        "import librosa.display\n",
        "import IPython.display as ipd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML, Video\n",
        "\n",
        "import random\n",
        "\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "import gc\n",
        "\n",
        "# !pip install pytorchvideo\n",
        "\n",
        "from torchvision.transforms import Compose, Lambda, RandomCrop, RandomHorizontalFlip, Resize, ToTensor, ToPILImage, CenterCrop, ColorJitter, RandomPerspective\n",
        "\n",
        "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "#     for filename in filenames:\n",
        "#         print(os.path.join(dirname, filename))\n",
        "\n",
        "import inspect"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f93fe363",
      "metadata": {
        "id": "f93fe363",
        "papermill": {
          "duration": 0.041627,
          "end_time": "2024-09-19T03:39:52.583533",
          "exception": false,
          "start_time": "2024-09-19T03:39:52.541906",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## **Setting up environment & hyperparameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 337,
      "id": "05b8e1df",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-19T03:39:52.837053Z",
          "iopub.status.busy": "2024-09-19T03:39:52.836599Z",
          "iopub.status.idle": "2024-09-19T03:39:56.541561Z",
          "shell.execute_reply": "2024-09-19T03:39:56.539659Z"
        },
        "papermill": {
          "duration": 3.748069,
          "end_time": "2024-09-19T03:39:56.545327",
          "exception": false,
          "start_time": "2024-09-19T03:39:52.797258",
          "status": "completed"
        },
        "tags": [],
        "id": "05b8e1df"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import wandb\n",
        "    wandb.login()\n",
        "except:\n",
        "    !pip install wandb -q\n",
        "    import wandb\n",
        "    wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 338,
      "id": "9dd6eba9",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-19T03:39:56.630034Z",
          "iopub.status.busy": "2024-09-19T03:39:56.628769Z",
          "iopub.status.idle": "2024-09-19T03:39:56.640016Z",
          "shell.execute_reply": "2024-09-19T03:39:56.638082Z"
        },
        "id": "9dd6eba9",
        "outputId": "6d22b0e2-3106-4cd0-f3b8-7f0db33b967c",
        "papermill": {
          "duration": 0.059384,
          "end_time": "2024-09-19T03:39:56.643818",
          "exception": false,
          "start_time": "2024-09-19T03:39:56.584434",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 338
        }
      ],
      "source": [
        "# Set up device: use GPU or CPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 339,
      "id": "e7b4a182",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-19T03:39:56.821830Z",
          "iopub.status.busy": "2024-09-19T03:39:56.821314Z",
          "iopub.status.idle": "2024-09-19T03:39:56.829077Z",
          "shell.execute_reply": "2024-09-19T03:39:56.827504Z"
        },
        "id": "e7b4a182",
        "papermill": {
          "duration": 0.055554,
          "end_time": "2024-09-19T03:39:56.832568",
          "exception": false,
          "start_time": "2024-09-19T03:39:56.777014",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af429278-1e59-4614-e573-9d5daa60ef50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No CUDA. CPU available\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    torch.cuda.get_device_name(0)\n",
        "except:\n",
        "    print(\"No CUDA. CPU available\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 340,
      "id": "ef4c4306",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-19T03:39:56.920869Z",
          "iopub.status.busy": "2024-09-19T03:39:56.919838Z",
          "iopub.status.idle": "2024-09-19T03:39:56.926332Z",
          "shell.execute_reply": "2024-09-19T03:39:56.924682Z"
        },
        "id": "ef4c4306",
        "papermill": {
          "duration": 0.055549,
          "end_time": "2024-09-19T03:39:56.929645",
          "exception": false,
          "start_time": "2024-09-19T03:39:56.874096",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# The path to the root directory of the dataset. Change this on your system\n",
        "working_dir = \"/content/drive/MyDrive/datasets/CREMA-D/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 341,
      "id": "f6007a28",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-19T03:39:57.009657Z",
          "iopub.status.busy": "2024-09-19T03:39:57.008291Z",
          "iopub.status.idle": "2024-09-19T03:39:57.019909Z",
          "shell.execute_reply": "2024-09-19T03:39:57.018424Z"
        },
        "id": "f6007a28",
        "outputId": "12466047-bcf0-4476-f325-749578453ae3",
        "papermill": {
          "duration": 0.055375,
          "end_time": "2024-09-19T03:39:57.023585",
          "exception": false,
          "start_time": "2024-09-19T03:39:56.968210",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'lr': 0.0001,\n",
              " 'epochs': 30,\n",
              " 'adam_betas': (0.98, 0.999),\n",
              " 'batch': 16,\n",
              " 'sdg_momentum': 0.99,\n",
              " 'sdg_weight_decay': 0.45,\n",
              " 'num_features': 1024,\n",
              " 'max_seq_len': 120}"
            ]
          },
          "metadata": {},
          "execution_count": 341
        }
      ],
      "source": [
        "# Hyperparameters. Tweak as you wish\n",
        "hyperparams = {\n",
        "    \"lr\": 0.0001, # Learning Rate\n",
        "    \"epochs\": 30, # Number of Epochs\n",
        "    \"adam_betas\": (0.98, 0.999), # B1 and B2 (weight decays) of ADAM\n",
        "    \"batch\": 16, # Mini-batch size\n",
        "    \"sdg_momentum\": 0.99, # Stochastic Gradient Descent momentum\n",
        "    \"sdg_weight_decay\": 0.45, # Stochastic Gradient Descent weight decay,\n",
        "    \"num_features\": 1024, # the no. of feature maps we will divide the image into\n",
        "    \"max_seq_len\": 120\n",
        "}\n",
        "\n",
        "hyperparams"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb505d0c",
      "metadata": {
        "id": "bb505d0c",
        "papermill": {
          "duration": 0.038952,
          "end_time": "2024-09-19T03:39:57.104636",
          "exception": false,
          "start_time": "2024-09-19T03:39:57.065684",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "<br/>\n",
        "\n",
        "As stated by RAVDESS: \\\n",
        "\n",
        "File naming convention\n",
        "\n",
        "Each of the 1440 files has a unique filename. The filename consists of a 1-aprt numerical identifier (e.g., s03.wav). These identifiers define the stimulus characteristics:\n",
        "\n",
        "Filename identifiers:\n",
        "\n",
        "Audio files consist of audio WAV files sampled at 44.1 kHz\n",
        "\n",
        "There are 15 sentences for each of the 7 emotion categories.\n",
        "The initial letter(s) of the file name represents the emotion class, and the following digits represent the sentence number.\n",
        "The letters 'a', 'd', 'f', 'h', 'n', 'sa' and 'su' represent 'anger', 'disgust', 'fear', 'happiness', 'neutral', 'sadness' and 'surprise' emotion classes respectively.\n",
        "\n",
        "\n",
        "E.g., 'd03.wav' is the 3rd disgust sentence audio. \\\n",
        "E.g., 'd03.avi' is the 3rd disgust sentence video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 342,
      "id": "ec9bbd82",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-19T03:39:57.186238Z",
          "iopub.status.busy": "2024-09-19T03:39:57.185120Z",
          "iopub.status.idle": "2024-09-19T03:39:57.194507Z",
          "shell.execute_reply": "2024-09-19T03:39:57.192744Z"
        },
        "id": "ec9bbd82",
        "papermill": {
          "duration": 0.054628,
          "end_time": "2024-09-19T03:39:57.197659",
          "exception": false,
          "start_time": "2024-09-19T03:39:57.143031",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# A dict that maps the class name to our assigned index (uses: track emotion index for prediction)\n",
        "class2idx = {\n",
        "    \"anger\": 0,\n",
        "    \"disgust\": 1,\n",
        "    \"fear\": 2,\n",
        "    \"happy\": 3,\n",
        "    \"neutral\": 4,\n",
        "    \"sad\": 5,\n",
        "}\n",
        "\n",
        "# A dict that maps the index to the class name (uses: decorate prediction)\n",
        "idx2class = {v:k for k,v in class2idx.items()}\n",
        "\n",
        "# A dict that maps the type given in the file name to our index(uses: dataset preparation)\n",
        "tag2idx = {\n",
        "    \"ANG\": 0,\n",
        "    \"DIS\": 1,\n",
        "    \"FEA\": 2,\n",
        "    \"HAP\": 3,\n",
        "    \"NEU\": 4,\n",
        "    \"SAD\": 5,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 343,
      "id": "90c9559f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-19T03:39:57.285571Z",
          "iopub.status.busy": "2024-09-19T03:39:57.284275Z",
          "iopub.status.idle": "2024-09-19T03:39:57.294996Z",
          "shell.execute_reply": "2024-09-19T03:39:57.293159Z"
        },
        "id": "90c9559f",
        "outputId": "d7d8a6a9-c3a6-4602-ac4b-652b4ad0388d",
        "papermill": {
          "duration": 0.058071,
          "end_time": "2024-09-19T03:39:57.298405",
          "exception": false,
          "start_time": "2024-09-19T03:39:57.240334",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'anger', 1: 'disgust', 2: 'fear', 3: 'happy', 4: 'neutral', 5: 'sad'}"
            ]
          },
          "metadata": {},
          "execution_count": 343
        }
      ],
      "source": [
        "idx2class"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecd002ec",
      "metadata": {
        "id": "ecd002ec",
        "papermill": {
          "duration": 0.037512,
          "end_time": "2024-09-19T03:39:57.374873",
          "exception": false,
          "start_time": "2024-09-19T03:39:57.337361",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## **Defining the Transforms(Augmentation Techniques) and helper functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 344,
      "id": "ffd61a05",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-19T03:39:57.455283Z",
          "iopub.status.busy": "2024-09-19T03:39:57.454225Z",
          "iopub.status.idle": "2024-09-19T03:39:57.464463Z",
          "shell.execute_reply": "2024-09-19T03:39:57.462822Z"
        },
        "id": "ffd61a05",
        "papermill": {
          "duration": 0.054192,
          "end_time": "2024-09-19T03:39:57.467739",
          "exception": false,
          "start_time": "2024-09-19T03:39:57.413547",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Defining the transforms:\n",
        "video_frame_transform = Compose([\n",
        "#     ToPILImage(),\n",
        "    Resize((252,252)),\n",
        "    CenterCrop((184,184)),\n",
        "#     ToTensor()\n",
        "])\n",
        "\n",
        "# change frame color randomly\n",
        "video_frame_augment_color = Compose([\n",
        "#     ToPILImage(),\n",
        "    Resize((252,252)),\n",
        "    CenterCrop((184,184)),\n",
        "    ColorJitter(brightness=0.4, hue=0.3, saturation=0.4),\n",
        "#     ToTensor()\n",
        "])\n",
        "\n",
        "# change frame prespective randomly\n",
        "video_frame_augment_persp = Compose([\n",
        "#     ToPILImage(),\n",
        "    Resize((252,252)),\n",
        "    CenterCrop((184,184)),\n",
        "    RandomPerspective(distortion_scale=0.3, p=1.0),\n",
        "#     ToTensor()\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 345,
      "id": "4ad16d89",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-19T03:39:57.547355Z",
          "iopub.status.busy": "2024-09-19T03:39:57.546829Z",
          "iopub.status.idle": "2024-09-19T03:39:57.565398Z",
          "shell.execute_reply": "2024-09-19T03:39:57.563659Z"
        },
        "id": "4ad16d89",
        "papermill": {
          "duration": 0.06305,
          "end_time": "2024-09-19T03:39:57.569548",
          "exception": false,
          "start_time": "2024-09-19T03:39:57.506498",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "    Defining the helper functions for the Audio mel-spectogram technique\n",
        "\"\"\"\n",
        "\n",
        "# Get the melspec of the audio as image/np 2d array\n",
        "def wav2melSpec(AUDIO_PATH):\n",
        "    audio, sr = librosa.load(AUDIO_PATH)\n",
        "    return librosa.feature.melspectrogram(y=audio, sr=sr)\n",
        "\n",
        "\n",
        "# Show the image spectogram\n",
        "def imgSpec(ms_feature):\n",
        "    fig, ax = plt.subplots()\n",
        "    ms_dB = librosa.power_to_db(ms_feature, ref=np.max)\n",
        "    print(ms_feature.shape)\n",
        "    img = librosa.display.specshow(ms_dB, x_axis='time', y_axis='mel', ax=ax)\n",
        "    fig.colorbar(img, ax=ax, format='%+2.0f dB')\n",
        "    ax.set(title='Mel-frequency spectrogram');\n",
        "\n",
        "# Hear the audio\n",
        "def hear_audio(AUDIO_PATH):\n",
        "    audio, sr = librosa.load(AUDIO_PATH)\n",
        "\n",
        "    print(\"\\t\", end=\"\")\n",
        "    ipd.display(ipd.Audio(data=audio, rate=sr))\n",
        "\n",
        "\n",
        "def show_video(video_path):\n",
        "    from base64 import b64encode\n",
        "\n",
        "    if os.path.isfile(video_path):\n",
        "        ext = '.mp4'\n",
        "    else:\n",
        "        print(\"Error: Please check the path.\")\n",
        "\n",
        "    video_encoded = open(video_path, \"rb\").read()\n",
        "    data = \"data:video/mp4;base64,\" + b64encode(video_encoded).decode()\n",
        "\n",
        "    video_tag = '<video width=\"400\" height=\"300\" controls alt=\"test\" src=\"%s\">' % data\n",
        "    return HTML(data=video_tag)\n",
        "\n",
        "# Show 1 example\n",
        "def show_example(video_path, audio_path, prediction=None, actual=None, save_memory=False):\n",
        "    if prediction is not None:\n",
        "        print(\"Predicted Label:\", idx2class[prediction])\n",
        "    print(\"Actual Label:\", idx2class[actual])\n",
        "\n",
        "    if save_memory is False:\n",
        "        print(\"Video path:\", video_path)\n",
        "        ipd.display(Video(video_path, embed=True, width=400, height=300))\n",
        "\n",
        "        # display(show_video(video_path))\n",
        "        print(\"Audio path:\", audio_path)\n",
        "        hear_audio(audio_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 346,
      "id": "99c51a6d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-19T03:39:57.652447Z",
          "iopub.status.busy": "2024-09-19T03:39:57.651911Z",
          "iopub.status.idle": "2024-09-19T03:39:57.941130Z",
          "shell.execute_reply": "2024-09-19T03:39:57.939680Z"
        },
        "id": "99c51a6d",
        "papermill": {
          "duration": 0.334672,
          "end_time": "2024-09-19T03:39:57.944605",
          "exception": false,
          "start_time": "2024-09-19T03:39:57.609933",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "    Defining the helper functions for the Audio MFCC technique\n",
        "\"\"\"\n",
        "\n",
        "# audio effects\n",
        "def audio_effects(audio, sample_rate, augment=1):\n",
        "    data = None\n",
        "    if augment == 1:\n",
        "        data = librosa.effects.percussive(y=audio)\n",
        "    elif augment == 2:\n",
        "        data = librosa.effects.pitch_shift(y=audio, sr=sample_rate, n_steps=3)\n",
        "    return data\n",
        "\n",
        "\n",
        "# normalize the audio wave\n",
        "def normalize_audio(audio):\n",
        "    audio = audio / np.max(np.abs(audio))\n",
        "    return audio\n",
        "\n",
        "def feature_extractor(file, augment=0, test=False):\n",
        "\n",
        "    attempt = 0\n",
        "    while True:\n",
        "        try:\n",
        "            data, sample_rate = librosa.load(file)\n",
        "            break\n",
        "        except:\n",
        "            if attempt == 50:\n",
        "                print(\"failed trying to find audio file\", file)\n",
        "                break\n",
        "            print(\"Audio file not read. Trying again\")\n",
        "            attempt += 1\n",
        "\n",
        "#     print(data.shape)\n",
        "\n",
        "    if augment > 0:\n",
        "        data = audio_effects(data, sample_rate, augment=augment)\n",
        "\n",
        "    data = normalize_audio(data)\n",
        "\n",
        "    # zero crossing rate\n",
        "    zcr = librosa.feature.zero_crossing_rate(y=data)[0]\n",
        "    zcr /= zcr.max()\n",
        "    zcr = zcr[0:(0+128)]\n",
        "    if len(zcr) < 128:\n",
        "        zcr = librosa.util.fix_length(zcr, size=128)\n",
        "#     result=np.vstack((result, zcr))\n",
        "\n",
        "\n",
        "    # MFCC\n",
        "    mfcc = np.mean(librosa.feature.mfcc(y=data, sr=sample_rate, n_mfcc=128).T, axis=0)\n",
        "    mfcc /= mfcc.max()\n",
        "#     result = np.vstack((result, mfcc))\n",
        "\n",
        "    # Root Mean Square Value\n",
        "    rms = librosa.feature.rms(y=data)[0]\n",
        "    rms /= rms.max()\n",
        "    rms = rms[0:(0+128)]\n",
        "    if len(rms) < 128:\n",
        "        rms = librosa.util.fix_length(rms, size=128)\n",
        "#     result = np.vstack((result, rms))\n",
        "\n",
        "    # MelSpectogram\n",
        "    mel = librosa.feature.melspectrogram(y=data, sr=sample_rate)\n",
        "    mel = librosa.amplitude_to_db(mel, ref = np.max)\n",
        "    mel = np.mean(mel.T, axis=0)\n",
        "    mel /= mel.sum()\n",
        "#     result = np.vstack((result, mel))\n",
        "\n",
        "    if test:\n",
        "        return_dict = {\n",
        "            \"raw\": data,\n",
        "            \"sr\": sample_rate,\n",
        "            \"zcr\": zcr,\n",
        "            \"mfcc\": mfcc,\n",
        "            \"rms\": rms,\n",
        "            \"mel\": mel\n",
        "        }\n",
        "    else:\n",
        "        return_dict = {\n",
        "            \"zcr\": zcr,\n",
        "            \"mfcc\": mfcc,\n",
        "            \"rms\": rms,\n",
        "            \"mel\": mel\n",
        "        }\n",
        "    return return_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 347,
      "id": "ad9583e4",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-19T03:39:58.027608Z",
          "iopub.status.busy": "2024-09-19T03:39:58.026856Z",
          "iopub.status.idle": "2024-09-19T03:39:58.035289Z",
          "shell.execute_reply": "2024-09-19T03:39:58.033943Z"
        },
        "id": "ad9583e4",
        "papermill": {
          "duration": 0.053481,
          "end_time": "2024-09-19T03:39:58.038056",
          "exception": false,
          "start_time": "2024-09-19T03:39:57.984575",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "def dict_to_tensor(dictionary):\n",
        "    out_dict = {}\n",
        "    for item in dictionary.items():\n",
        "        if item[0] == \"sr\":\n",
        "            out_dict[item[0]] = torch.tensor(item[1]).to(device)\n",
        "        else:\n",
        "            out_dict[item[0]] = torch.from_numpy(item[1]).to(device)\n",
        "\n",
        "    return out_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0998118",
      "metadata": {
        "id": "a0998118",
        "papermill": {
          "duration": 0.039677,
          "end_time": "2024-09-19T03:39:58.117663",
          "exception": false,
          "start_time": "2024-09-19T03:39:58.077986",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "<br/>\n",
        "\n",
        "---\n",
        "\n",
        "<br/>\n",
        "\n",
        "## **Dataset Preparation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 348,
      "id": "c73535a1",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-19T03:39:58.203510Z",
          "iopub.status.busy": "2024-09-19T03:39:58.202940Z",
          "iopub.status.idle": "2024-09-19T03:39:58.218781Z",
          "shell.execute_reply": "2024-09-19T03:39:58.217181Z"
        },
        "id": "c73535a1",
        "papermill": {
          "duration": 0.064622,
          "end_time": "2024-09-19T03:39:58.222362",
          "exception": false,
          "start_time": "2024-09-19T03:39:58.157740",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "def make_ds_as_list(path):\n",
        "    audio = []\n",
        "    video = []\n",
        "    labels = []\n",
        "\n",
        "    # renamed folders\n",
        "    video_dir = os.path.join(path, \"VideoFlash\")\n",
        "    audio_dir = os.path.join(path, \"AudioWAV\")\n",
        "\n",
        "    video_files = sorted([f for f in os.listdir(video_dir) if f.endswith('.mp4')])\n",
        "    audio_files = sorted([f for f in os.listdir(audio_dir) if f.endswith('.wav')])\n",
        "\n",
        "    # lookup dictionary for audio files\n",
        "    audio_bases_dict = {os.path.splitext(f)[0]: f for f in audio_files}\n",
        "\n",
        "    matched_count = 0\n",
        "\n",
        "    for video_file in video_files:\n",
        "        video_base = os.path.splitext(video_file)[0]\n",
        "\n",
        "\n",
        "        # CREMA-D filenames are like: 1001_DFA_ANG_XX\n",
        "        prefix = '_'.join(video_base.split('_')[:4])\n",
        "\n",
        "        matching_audio = audio_bases_dict.get(prefix, None)\n",
        "\n",
        "        if matching_audio is None:\n",
        "            continue  # Skip if no match\n",
        "\n",
        "        # emotion tag is always 3rd chunk (e.g., \"ANG\")\n",
        "        label_tag = video_base.split('_')[2].upper()\n",
        "        label = tag2idx.get(label_tag, None)\n",
        "        if label is None:\n",
        "            continue\n",
        "\n",
        "        video.append(os.path.join(video_dir, video_file))\n",
        "        audio.append(os.path.join(audio_dir, matching_audio))\n",
        "        labels.append(label)\n",
        "        matched_count += 1\n",
        "\n",
        "    print(f\"DEBUG: {matched_count} matching audio-video-label triplets created.\")\n",
        "\n",
        "    return audio, video, labels\n",
        "\n",
        "\n",
        "def make_dataframe(path):\n",
        "    audio, video, labels = make_ds_as_list(path)\n",
        "    df = pd.DataFrame({\n",
        "        \"audio_path\": audio,\n",
        "        \"video_path\": video,\n",
        "        \"label\": labels,\n",
        "        \"augment\": [0] * len(labels)   # default augment flag\n",
        "    })\n",
        "    return df\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 349,
      "id": "a7735f96",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-19T03:39:58.310473Z",
          "iopub.status.busy": "2024-09-19T03:39:58.309875Z",
          "iopub.status.idle": "2024-09-19T03:40:04.716750Z",
          "shell.execute_reply": "2024-09-19T03:40:04.715436Z"
        },
        "id": "a7735f96",
        "papermill": {
          "duration": 6.454711,
          "end_time": "2024-09-19T03:40:04.720291",
          "exception": false,
          "start_time": "2024-09-19T03:39:58.265580",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8c6ea93-c526-4c47-c82f-5ac19c9e0eab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: 7442 matching audio-video-label triplets created.\n",
            "Total samples collected: 7442\n",
            "                                          audio_path  \\\n",
            "0  /content/drive/MyDrive/datasets/CREMA-D/AudioW...   \n",
            "1  /content/drive/MyDrive/datasets/CREMA-D/AudioW...   \n",
            "2  /content/drive/MyDrive/datasets/CREMA-D/AudioW...   \n",
            "3  /content/drive/MyDrive/datasets/CREMA-D/AudioW...   \n",
            "4  /content/drive/MyDrive/datasets/CREMA-D/AudioW...   \n",
            "\n",
            "                                          video_path  label  augment  \n",
            "0  /content/drive/MyDrive/datasets/CREMA-D/VideoF...      0        0  \n",
            "1  /content/drive/MyDrive/datasets/CREMA-D/VideoF...      1        0  \n",
            "2  /content/drive/MyDrive/datasets/CREMA-D/VideoF...      2        0  \n",
            "3  /content/drive/MyDrive/datasets/CREMA-D/VideoF...      3        0  \n",
            "4  /content/drive/MyDrive/datasets/CREMA-D/VideoF...      4        0  \n",
            "Train: 4465, Validation: 1488, Test: 1489\n"
          ]
        }
      ],
      "source": [
        "# Make the dataset\n",
        "df = make_dataframe(working_dir)\n",
        "\n",
        "# Check if data is valid\n",
        "print(f\"Total samples collected: {len(df)}\")\n",
        "print(df.head())\n",
        "\n",
        "# Proceed with train-test split only if data is available\n",
        "if len(df) > 0:\n",
        "    from sklearn.model_selection import train_test_split\n",
        "\n",
        "    train_df, temp_df = train_test_split(df, test_size=0.40, random_state=42)\n",
        "    cv_df, test_df = train_test_split(temp_df, test_size=0.50, random_state=42)\n",
        "\n",
        "    train_df['augment'] = 0\n",
        "    cv_df['augment'] = 0\n",
        "    test_df['augment'] = 0\n",
        "\n",
        "    print(f\"Train: {len(train_df)}, Validation: {len(cv_df)}, Test: {len(test_df)}\")\n",
        "\n",
        "else:\n",
        "    raise Exception(\"No data found. Please check folder names and file format.\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38d8e265",
      "metadata": {
        "id": "38d8e265",
        "papermill": {
          "duration": 0.039611,
          "end_time": "2024-09-19T03:40:04.908236",
          "exception": false,
          "start_time": "2024-09-19T03:40:04.868625",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "<br>\n",
        "<hr>\n",
        "\n",
        "## Data checking and testing phase"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc628e5a",
      "metadata": {
        "id": "bc628e5a",
        "papermill": {
          "duration": 0.040153,
          "end_time": "2024-09-19T03:40:04.991124",
          "exception": false,
          "start_time": "2024-09-19T03:40:04.950971",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "### Do the transforms work?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 350,
      "id": "cc47c199",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-19T03:40:05.072820Z",
          "iopub.status.busy": "2024-09-19T03:40:05.072387Z",
          "iopub.status.idle": "2024-09-19T03:40:05.079823Z",
          "shell.execute_reply": "2024-09-19T03:40:05.078252Z"
        },
        "id": "cc47c199",
        "papermill": {
          "duration": 0.051751,
          "end_time": "2024-09-19T03:40:05.082863",
          "exception": false,
          "start_time": "2024-09-19T03:40:05.031112",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# idx = 50\n",
        "\n",
        "\n",
        "# # RAW AUDIO\n",
        "# d = feature_extractor(non_augment_df[\"audio_path\"][idx], augment=0, test=True)\n",
        "# data = d[\"raw\"]\n",
        "# sr = d[\"sr\"]\n",
        "# print(\"Audio transformed with augmentation scheme 0 (no augementation; raw audio):\")\n",
        "# ipd.display(ipd.Audio(data=data, rate=sr))\n",
        "# print(\"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "# # AUGMENTATION 1 - harmonic\n",
        "# d = feature_extractor(augment_1_df[\"audio_path\"][idx], augment=1, test=True)\n",
        "# data = d[\"raw\"]\n",
        "# sr = d[\"sr\"]\n",
        "# print(\"Audio transformed with augmentation scheme 1:\")\n",
        "# ipd.display(ipd.Audio(data=data, rate=sr))\n",
        "# print(\"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "# # AUGMENTATION 2 - pitch shift\n",
        "# d = feature_extractor(augment_2_df[\"audio_path\"][idx], augment=2, test=True)\n",
        "# data = d[\"raw\"]\n",
        "# sr = d[\"sr\"]\n",
        "# print(\"Audio transformed with augmentation scheme 2:\")\n",
        "# ipd.display(ipd.Audio(data=data, rate=sr))\n",
        "# print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 350,
      "id": "48a9c7b3",
      "metadata": {
        "papermill": {
          "duration": 0.040612,
          "end_time": "2024-09-19T03:40:05.163675",
          "exception": false,
          "start_time": "2024-09-19T03:40:05.123063",
          "status": "completed"
        },
        "tags": [],
        "id": "48a9c7b3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "64c5c5db",
      "metadata": {
        "id": "64c5c5db",
        "papermill": {
          "duration": 0.040284,
          "end_time": "2024-09-19T03:40:06.007384",
          "exception": false,
          "start_time": "2024-09-19T03:40:05.967100",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "<br/>\n",
        "\n",
        "---\n",
        "\n",
        "## **Dataset Splitting**\n",
        "\n",
        "We will now split the dataset into train, cv, test splits.\n",
        "\n",
        "60:20:20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 351,
      "id": "a2b6bbc8",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-19T03:40:06.091651Z",
          "iopub.status.busy": "2024-09-19T03:40:06.091072Z",
          "iopub.status.idle": "2024-09-19T03:40:06.116828Z",
          "shell.execute_reply": "2024-09-19T03:40:06.115258Z"
        },
        "id": "a2b6bbc8",
        "outputId": "b5fd5454-9d4a-4f5b-944d-08bf5f7007ed",
        "papermill": {
          "duration": 0.072158,
          "end_time": "2024-09-19T03:40:06.120146",
          "exception": false,
          "start_time": "2024-09-19T03:40:06.047988",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4465, 2977)"
            ]
          },
          "metadata": {},
          "execution_count": 351
        }
      ],
      "source": [
        "# Split into 60% train, 20% val, 20% test set\n",
        "train_df, test_df = train_test_split(df, test_size=0.40, random_state=42)\n",
        "len(train_df), len(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 352,
      "id": "7264f2db",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7264f2db",
        "outputId": "ba0e980e-1274-4c4f-a7bc-a6d66466c92f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/datasets/CREMA-D/AudioWAV/1035_ITS_DIS_XX.wav /content/drive/MyDrive/datasets/CREMA-D/VideoFlash/1035_ITS_DIS_XX.mp4 /content/drive/MyDrive/datasets/CREMA-D/AudioWAV/1035_ITS_DIS_XX.wav /content/drive/MyDrive/datasets/CREMA-D/VideoFlash/1035_ITS_DIS_XX.mp4\n"
          ]
        }
      ],
      "source": [
        "# Create augmented copies\n",
        "augment_1_df = train_df.copy().reset_index(drop=True)\n",
        "augment_1_df[\"augment\"] = 1\n",
        "\n",
        "augment_2_df = train_df.copy().reset_index(drop=True)\n",
        "augment_2_df[\"augment\"] = 2\n",
        "\n",
        "# Reset index on the original training set too\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "train_df[\"augment\"] = 0\n",
        "\n",
        "# âœ… Safe way to check an example row\n",
        "print(\n",
        "    train_df[\"audio_path\"].iloc[0],\n",
        "    train_df[\"video_path\"].iloc[0],\n",
        "    augment_1_df[\"audio_path\"].iloc[0],\n",
        "    augment_1_df[\"video_path\"].iloc[0]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 353,
      "id": "6c8f14a8",
      "metadata": {
        "id": "6c8f14a8"
      },
      "outputs": [],
      "source": [
        "train_df = pd.concat([train_df, augment_1_df, augment_2_df])\n",
        "train_df.head()\n",
        "train_df = train_df.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 354,
      "id": "f7a6540d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-19T03:40:06.204646Z",
          "iopub.status.busy": "2024-09-19T03:40:06.204057Z",
          "iopub.status.idle": "2024-09-19T03:40:06.210746Z",
          "shell.execute_reply": "2024-09-19T03:40:06.209253Z"
        },
        "id": "f7a6540d",
        "papermill": {
          "duration": 0.051849,
          "end_time": "2024-09-19T03:40:06.213588",
          "exception": false,
          "start_time": "2024-09-19T03:40:06.161739",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# # Check your examples\n",
        "# idx = 90 # Change index to see different examples\n",
        "# show_example(train_df[\"video_path\"].iloc[idx], train_df[\"audio_path\"].iloc[idx], actual=train_df[\"label\"].iloc[idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 355,
      "id": "ca10c9a2",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-19T03:40:06.299918Z",
          "iopub.status.busy": "2024-09-19T03:40:06.299456Z",
          "iopub.status.idle": "2024-09-19T03:40:06.311807Z",
          "shell.execute_reply": "2024-09-19T03:40:06.310611Z"
        },
        "id": "ca10c9a2",
        "papermill": {
          "duration": 0.060183,
          "end_time": "2024-09-19T03:40:06.314896",
          "exception": false,
          "start_time": "2024-09-19T03:40:06.254713",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "cv_df, test_df = train_test_split(test_df, test_size=0.50, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 356,
      "id": "bb22cb11",
      "metadata": {
        "id": "bb22cb11"
      },
      "outputs": [],
      "source": [
        "cv_df['augment'] = 0\n",
        "test_df['augment'] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 357,
      "id": "7c836c09",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-19T03:40:06.400253Z",
          "iopub.status.busy": "2024-09-19T03:40:06.399814Z",
          "iopub.status.idle": "2024-09-19T03:40:06.408806Z",
          "shell.execute_reply": "2024-09-19T03:40:06.407325Z"
        },
        "id": "7c836c09",
        "outputId": "345fa9bc-af02-4b80-af8e-8ea88b634a08",
        "papermill": {
          "duration": 0.054561,
          "end_time": "2024-09-19T03:40:06.411616",
          "exception": false,
          "start_time": "2024-09-19T03:40:06.357055",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13395, 1488, 1489)"
            ]
          },
          "metadata": {},
          "execution_count": 357
        }
      ],
      "source": [
        "# View their length\n",
        "len(train_df), len(cv_df), len(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 358,
      "id": "840505ac",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-19T03:40:06.495756Z",
          "iopub.status.busy": "2024-09-19T03:40:06.495274Z",
          "iopub.status.idle": "2024-09-19T03:40:06.761184Z",
          "shell.execute_reply": "2024-09-19T03:40:06.760000Z"
        },
        "id": "840505ac",
        "outputId": "f8036cb6-9ee2-4241-9275-8fb0e88b1c37",
        "papermill": {
          "duration": 0.311182,
          "end_time": "2024-09-19T03:40:06.764052",
          "exception": false,
          "start_time": "2024-09-19T03:40:06.452870",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4152"
            ]
          },
          "metadata": {},
          "execution_count": 358
        }
      ],
      "source": [
        "del df\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 359,
      "id": "7cf0067f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-19T03:40:06.849497Z",
          "iopub.status.busy": "2024-09-19T03:40:06.849026Z",
          "iopub.status.idle": "2024-09-19T03:40:06.854749Z",
          "shell.execute_reply": "2024-09-19T03:40:06.853530Z"
        },
        "id": "7cf0067f",
        "papermill": {
          "duration": 0.051688,
          "end_time": "2024-09-19T03:40:06.857656",
          "exception": false,
          "start_time": "2024-09-19T03:40:06.805968",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# import torchvision.models as models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 360,
      "id": "65214e54",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-19T03:40:06.942077Z",
          "iopub.status.busy": "2024-09-19T03:40:06.941401Z",
          "iopub.status.idle": "2024-09-19T03:40:06.948528Z",
          "shell.execute_reply": "2024-09-19T03:40:06.946273Z"
        },
        "papermill": {
          "duration": 0.053297,
          "end_time": "2024-09-19T03:40:06.951782",
          "exception": false,
          "start_time": "2024-09-19T03:40:06.898485",
          "status": "completed"
        },
        "tags": [],
        "id": "65214e54"
      },
      "outputs": [],
      "source": [
        "# videodata = skvideo.io.vread(\"/kaggle/input/cremad/CREMA-D/VideoFlash/1066_IOM_DIS_XX.mp4\")\n",
        "# print(videodata.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 361,
      "id": "86ea3fc8",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-19T03:40:07.033728Z",
          "iopub.status.busy": "2024-09-19T03:40:07.033265Z",
          "iopub.status.idle": "2024-09-19T03:40:07.039228Z",
          "shell.execute_reply": "2024-09-19T03:40:07.037821Z"
        },
        "papermill": {
          "duration": 0.050409,
          "end_time": "2024-09-19T03:40:07.042263",
          "exception": false,
          "start_time": "2024-09-19T03:40:06.991854",
          "status": "completed"
        },
        "tags": [],
        "id": "86ea3fc8"
      },
      "outputs": [],
      "source": [
        "# videodata = torch.Tensor(videodata)\n",
        "# videodata /= 255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 362,
      "id": "f070a205",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-19T03:40:07.148151Z",
          "iopub.status.busy": "2024-09-19T03:40:07.147626Z",
          "iopub.status.idle": "2024-09-19T03:40:07.154790Z",
          "shell.execute_reply": "2024-09-19T03:40:07.153149Z"
        },
        "papermill": {
          "duration": 0.073012,
          "end_time": "2024-09-19T03:40:07.158139",
          "exception": false,
          "start_time": "2024-09-19T03:40:07.085127",
          "status": "completed"
        },
        "tags": [],
        "id": "f070a205"
      },
      "outputs": [],
      "source": [
        "# for i, frame in enumerate(videodata):\n",
        "#     frame = frame.numpy()\n",
        "#     plt.imshow(frame)\n",
        "#     plt.show()\n",
        "\n",
        "#     if i == 10:\n",
        "#         break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 364,
      "id": "7dc2aff5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-19T03:40:07.255573Z",
          "iopub.status.busy": "2024-09-19T03:40:07.255052Z",
          "iopub.status.idle": "2024-09-19T03:40:07.287692Z",
          "shell.execute_reply": "2024-09-19T03:40:07.286464Z"
        },
        "id": "7dc2aff5",
        "papermill": {
          "duration": 0.088439,
          "end_time": "2024-09-19T03:40:07.291758",
          "exception": false,
          "start_time": "2024-09-19T03:40:07.203319",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "random.seed(42)\n",
        "\n",
        "class CREMADataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dataframe,\n",
        "        video_frame_transform=None,\n",
        "        video_strategy='optimal',\n",
        "        cut_video=False,\n",
        "        cut_audio=False,\n",
        "        selection=\"quartile\",\n",
        "        variant=\"all\"\n",
        "    ):\n",
        "        self.cut_video = cut_video\n",
        "        self.cut_audio = cut_audio\n",
        "\n",
        "        self.examples = dataframe\n",
        "        self.video_frame_transform = {\n",
        "            0: video_frame_transform,\n",
        "            1: video_frame_augment_color,\n",
        "            2: video_frame_augment_persp\n",
        "        }\n",
        "\n",
        "        self.video_strategy = video_strategy\n",
        "        self.selection = selection\n",
        "        self.variant = variant\n",
        "\n",
        "        del dataframe, video_frame_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.examples.iloc[idx]\n",
        "        video_path = row['video_path']\n",
        "        audio_path = row['audio_path']\n",
        "        label = row['label']\n",
        "        augment = row['augment']\n",
        "\n",
        "        # Load video\n",
        "        video_data = skvideo.io.vread(video_path)  # [frames, H, W, C]\n",
        "        video_data = torch.tensor(video_data).float() / 255.0  # Normalize\n",
        "\n",
        "        # Select video frames\n",
        "        if self.video_strategy == 'optimal':\n",
        "            video_frames = self.__optimal_strategy(video_data, augment=augment)\n",
        "        else:\n",
        "            video_frames = self.__all_strategy(video_data)\n",
        "\n",
        "        # Extract audio features\n",
        "        audio_features = feature_extractor(audio_path, augment=augment, test=False)\n",
        "        audio_tensor = dict_to_tensor(audio_features)\n",
        "\n",
        "        return video_frames, audio_tensor, label, video_path, audio_path\n",
        "\n",
        "    def __optimal_strategy(self, video, augment=0):\n",
        "        frames = []\n",
        "\n",
        "        q1_point = video.shape[0] // 4\n",
        "        q2_point = video.shape[0] // 2\n",
        "        q3_point = int((video.shape[0] * (3 / 4)))\n",
        "\n",
        "        q1_q2_mid = int((q1_point + (q2_point - 5)) // 2)\n",
        "        q2_q3_mid = int((q2_point + (q3_point - 5)) // 2)\n",
        "\n",
        "        q1_lb = q1_point - 10\n",
        "        q1_up = q1_q2_mid\n",
        "        q2_lb = q2_point - 10\n",
        "        q2_up = q2_q3_mid\n",
        "        q3_lb = q3_point - 10\n",
        "\n",
        "        six_frame_lower_bound = (self.selection == \"six\" and self.variant == \"lower\")\n",
        "        six_frame_upper_bound = (self.selection == \"six\" and self.variant == \"upper\")\n",
        "        original_strategy = (self.selection == \"quartile\" and self.variant == \"all\")\n",
        "\n",
        "        # Helper function to process a single frame\n",
        "        def process_frame(frame):\n",
        "            frame = torch.permute(frame, (2, 0, 1))  # [C, H, W]\n",
        "            return self.video_frame_transform[augment](frame)\n",
        "\n",
        "        if six_frame_lower_bound or original_strategy:\n",
        "            frames.append(process_frame(video[q1_lb]))\n",
        "\n",
        "        frames.append(process_frame(video[q1_point]))\n",
        "\n",
        "        if six_frame_upper_bound or original_strategy:\n",
        "            frames.append(process_frame(video[q1_up]))\n",
        "\n",
        "        if six_frame_lower_bound or original_strategy:\n",
        "            frames.append(process_frame(video[q2_lb]))\n",
        "\n",
        "        frames.append(process_frame(video[q2_point]))\n",
        "\n",
        "        if six_frame_upper_bound or original_strategy:\n",
        "            frames.append(process_frame(video[q2_up]))\n",
        "\n",
        "        if six_frame_lower_bound or original_strategy:\n",
        "            frames.append(process_frame(video[q3_lb]))\n",
        "\n",
        "        frames.append(process_frame(video[q3_point]))\n",
        "\n",
        "        if six_frame_upper_bound or original_strategy:\n",
        "            frames.append(process_frame(video[-1]))\n",
        "\n",
        "        frames = torch.stack(frames)  # Shape: [num_frames, C, H, W]\n",
        "        return frames\n",
        "\n",
        "    def __all_strategy(self, video):\n",
        "        length = video.shape[0]\n",
        "        frames = []\n",
        "\n",
        "        for i, f in enumerate(video):\n",
        "            if i == hyperparams[\"max_seq_len\"]:\n",
        "                break\n",
        "            frame = torch.permute(f, (2, 0, 1))  # [C, H, W]\n",
        "            frame = self.video_frame_transform[0](frame)\n",
        "            frames.append(frame)\n",
        "            del frame\n",
        "            gc.collect()\n",
        "\n",
        "        if length < hyperparams[\"max_seq_len\"]:\n",
        "            diff = hyperparams[\"max_seq_len\"] - length\n",
        "            Q2 = int(length // 2)\n",
        "            for i in range(Q2, Q2 + diff):\n",
        "                frame = torch.permute(video[i], (2, 0, 1))\n",
        "                frame = self.video_frame_transform[0](frame)\n",
        "                frames.append(frame)\n",
        "\n",
        "        frames = torch.stack(frames)\n",
        "        del video\n",
        "        gc.collect()\n",
        "        return frames\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 365,
      "id": "d1a687d7",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-19T03:40:07.408457Z",
          "iopub.status.busy": "2024-09-19T03:40:07.407910Z",
          "iopub.status.idle": "2024-09-19T03:40:07.417720Z",
          "shell.execute_reply": "2024-09-19T03:40:07.416447Z"
        },
        "id": "d1a687d7",
        "papermill": {
          "duration": 0.062558,
          "end_time": "2024-09-19T03:40:07.421590",
          "exception": false,
          "start_time": "2024-09-19T03:40:07.359032",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "trainds = CREMADataset(train_df, video_frame_transform)\n",
        "cvds = CREMADataset(cv_df, video_frame_transform)\n",
        "testds = CREMADataset(test_df, video_frame_transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 366,
      "id": "e0e7e955",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-19T03:40:07.517582Z",
          "iopub.status.busy": "2024-09-19T03:40:07.516420Z",
          "iopub.status.idle": "2024-09-19T03:40:07.525162Z",
          "shell.execute_reply": "2024-09-19T03:40:07.523647Z"
        },
        "id": "e0e7e955",
        "papermill": {
          "duration": 0.059966,
          "end_time": "2024-09-19T03:40:07.528395",
          "exception": false,
          "start_time": "2024-09-19T03:40:07.468429",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "trainloader = DataLoader(trainds, batch_size=hyperparams[\"batch\"], shuffle=True)\n",
        "cvloader = DataLoader(cvds, batch_size=hyperparams[\"batch\"], shuffle=False)\n",
        "testloader = DataLoader(testds, batch_size=hyperparams[\"batch\"], shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 367,
      "id": "33decd2a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-19T03:40:07.624910Z",
          "iopub.status.busy": "2024-09-19T03:40:07.624124Z",
          "iopub.status.idle": "2024-09-19T03:40:07.920808Z",
          "shell.execute_reply": "2024-09-19T03:40:07.919251Z"
        },
        "id": "33decd2a",
        "outputId": "b6e23f33-cc52-43ea-f8ad-50fc38ce2529",
        "papermill": {
          "duration": 0.351108,
          "end_time": "2024-09-19T03:40:07.924433",
          "exception": false,
          "start_time": "2024-09-19T03:40:07.573325",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1492"
            ]
          },
          "metadata": {},
          "execution_count": 367
        }
      ],
      "source": [
        "del trainds\n",
        "del cvds\n",
        "del testds\n",
        "del train_df\n",
        "del cv_df\n",
        "del test_df\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 368,
      "id": "eea1e567",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-19T03:40:08.020316Z",
          "iopub.status.busy": "2024-09-19T03:40:08.019798Z",
          "iopub.status.idle": "2024-09-19T03:40:08.030004Z",
          "shell.execute_reply": "2024-09-19T03:40:08.028300Z"
        },
        "id": "eea1e567",
        "papermill": {
          "duration": 0.061125,
          "end_time": "2024-09-19T03:40:08.033128",
          "exception": false,
          "start_time": "2024-09-19T03:40:07.972003",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "def view_a_loader(item, i):\n",
        "    video, audio, label, video_p, audio_p = item\n",
        "    show_example(video_p[i], audio_p[i], label[i].item(), label[i].item())\n",
        "    print(f\"Video shape: {video.shape} | Audio shape: {audio['mel'].shape}\")\n",
        "    print(f\"{video_p[i]}\")\n",
        "    for f in video[i]:\n",
        "        print(f[0])\n",
        "        f = torch.permute(f, (1,2,0))\n",
        "        plt.figure(figsize=(3, 3))\n",
        "        plt.imshow(f.numpy())\n",
        "        plt.show()\n",
        "#     imgSpec(audio[i].squeeze())\n",
        "\n",
        "    del item, video, audio, label, video_p, audio_p\n",
        "    gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a49a16e",
      "metadata": {
        "id": "4a49a16e",
        "papermill": {
          "duration": 0.040658,
          "end_time": "2024-09-19T03:40:08.406602",
          "exception": false,
          "start_time": "2024-09-19T03:40:08.365944",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## **The Model**\n",
        "\n",
        "It's time to build the model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cdeedf2",
      "metadata": {
        "id": "3cdeedf2",
        "papermill": {
          "duration": 0.040863,
          "end_time": "2024-09-19T03:40:08.488618",
          "exception": false,
          "start_time": "2024-09-19T03:40:08.447755",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "### The Video Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 369,
      "id": "38ad65ad",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-19T03:40:08.572531Z",
          "iopub.status.busy": "2024-09-19T03:40:08.571905Z",
          "iopub.status.idle": "2024-09-19T03:40:10.320953Z",
          "shell.execute_reply": "2024-09-19T03:40:10.319471Z"
        },
        "id": "38ad65ad",
        "papermill": {
          "duration": 1.795643,
          "end_time": "2024-09-19T03:40:10.324822",
          "exception": false,
          "start_time": "2024-09-19T03:40:08.529179",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "from torchvision.models.video import r2plus1d_18\n",
        "\n",
        "R2plus1D = r2plus1d_18(weights='KINETICS400_V1').to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 370,
      "id": "d14e0b46",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-19T03:40:10.425160Z",
          "iopub.status.busy": "2024-09-19T03:40:10.424710Z",
          "iopub.status.idle": "2024-09-19T03:40:10.439419Z",
          "shell.execute_reply": "2024-09-19T03:40:10.437579Z"
        },
        "id": "d14e0b46",
        "outputId": "b3818c9a-89c5-435f-b656-8f0e3551fc4d",
        "papermill": {
          "duration": 0.071849,
          "end_time": "2024-09-19T03:40:10.443391",
          "exception": false,
          "start_time": "2024-09-19T03:40:10.371542",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VideoResNet(\n",
              "  (stem): R2Plus1dStem(\n",
              "    (0): Conv3d(3, 45, kernel_size=(1, 7, 7), stride=(1, 2, 2), padding=(0, 3, 3), bias=False)\n",
              "    (1): BatchNorm3d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Conv3d(45, 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
              "    (4): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "  )\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv2Plus1D(\n",
              "          (0): Conv3d(64, 144, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
              "          (1): BatchNorm3d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "          (3): Conv3d(144, 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
              "        )\n",
              "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv2Plus1D(\n",
              "          (0): Conv3d(64, 144, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
              "          (1): BatchNorm3d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "          (3): Conv3d(144, 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
              "        )\n",
              "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv2Plus1D(\n",
              "          (0): Conv3d(64, 144, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
              "          (1): BatchNorm3d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "          (3): Conv3d(144, 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
              "        )\n",
              "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv2Plus1D(\n",
              "          (0): Conv3d(64, 144, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
              "          (1): BatchNorm3d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "          (3): Conv3d(144, 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
              "        )\n",
              "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv2Plus1D(\n",
              "          (0): Conv3d(64, 230, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)\n",
              "          (1): BatchNorm3d(230, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "          (3): Conv3d(230, 128, kernel_size=(3, 1, 1), stride=(2, 1, 1), padding=(1, 0, 0), bias=False)\n",
              "        )\n",
              "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv2Plus1D(\n",
              "          (0): Conv3d(128, 230, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
              "          (1): BatchNorm3d(230, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "          (3): Conv3d(230, 128, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
              "        )\n",
              "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
              "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv2Plus1D(\n",
              "          (0): Conv3d(128, 288, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
              "          (1): BatchNorm3d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "          (3): Conv3d(288, 128, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
              "        )\n",
              "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv2Plus1D(\n",
              "          (0): Conv3d(128, 288, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
              "          (1): BatchNorm3d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "          (3): Conv3d(288, 128, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
              "        )\n",
              "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv2Plus1D(\n",
              "          (0): Conv3d(128, 460, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)\n",
              "          (1): BatchNorm3d(460, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "          (3): Conv3d(460, 256, kernel_size=(3, 1, 1), stride=(2, 1, 1), padding=(1, 0, 0), bias=False)\n",
              "        )\n",
              "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv2Plus1D(\n",
              "          (0): Conv3d(256, 460, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
              "          (1): BatchNorm3d(460, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "          (3): Conv3d(460, 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
              "        )\n",
              "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
              "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv2Plus1D(\n",
              "          (0): Conv3d(256, 576, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
              "          (1): BatchNorm3d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "          (3): Conv3d(576, 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
              "        )\n",
              "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv2Plus1D(\n",
              "          (0): Conv3d(256, 576, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
              "          (1): BatchNorm3d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "          (3): Conv3d(576, 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
              "        )\n",
              "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv2Plus1D(\n",
              "          (0): Conv3d(256, 921, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)\n",
              "          (1): BatchNorm3d(921, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "          (3): Conv3d(921, 512, kernel_size=(3, 1, 1), stride=(2, 1, 1), padding=(1, 0, 0), bias=False)\n",
              "        )\n",
              "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv2Plus1D(\n",
              "          (0): Conv3d(512, 921, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
              "          (1): BatchNorm3d(921, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "          (3): Conv3d(921, 512, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
              "        )\n",
              "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
              "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv2Plus1D(\n",
              "          (0): Conv3d(512, 1152, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
              "          (1): BatchNorm3d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "          (3): Conv3d(1152, 512, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
              "        )\n",
              "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv2Plus1D(\n",
              "          (0): Conv3d(512, 1152, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
              "          (1): BatchNorm3d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "          (3): Conv3d(1152, 512, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
              "        )\n",
              "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool3d(output_size=(1, 1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=400, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 370
        }
      ],
      "source": [
        "R2plus1D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 371,
      "id": "a3364398",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-19T03:40:10.540594Z",
          "iopub.status.busy": "2024-09-19T03:40:10.540014Z",
          "iopub.status.idle": "2024-09-19T03:40:10.546989Z",
          "shell.execute_reply": "2024-09-19T03:40:10.545430Z"
        },
        "id": "a3364398",
        "papermill": {
          "duration": 0.060708,
          "end_time": "2024-09-19T03:40:10.550618",
          "exception": false,
          "start_time": "2024-09-19T03:40:10.489910",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "from torchvision.models.video.resnet import Conv2Plus1D"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "258d5424",
      "metadata": {
        "id": "258d5424",
        "papermill": {
          "duration": 0.041541,
          "end_time": "2024-09-19T03:40:10.636545",
          "exception": false,
          "start_time": "2024-09-19T03:40:10.595004",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "<hr>\n",
        "<br>\n",
        "\n",
        "\n",
        "### The Audio Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 372,
      "id": "8e8b198a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-19T03:40:10.837073Z",
          "iopub.status.busy": "2024-09-19T03:40:10.836573Z",
          "iopub.status.idle": "2024-09-19T03:40:10.843884Z",
          "shell.execute_reply": "2024-09-19T03:40:10.842569Z"
        },
        "id": "8e8b198a",
        "papermill": {
          "duration": 0.059097,
          "end_time": "2024-09-19T03:40:10.847079",
          "exception": false,
          "start_time": "2024-09-19T03:40:10.787982",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "class PassThrough(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 373,
      "id": "ddb7f55d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-19T03:40:11.069811Z",
          "iopub.status.busy": "2024-09-19T03:40:11.069369Z",
          "iopub.status.idle": "2024-09-19T03:40:11.085104Z",
          "shell.execute_reply": "2024-09-19T03:40:11.083432Z"
        },
        "id": "ddb7f55d",
        "papermill": {
          "duration": 0.070406,
          "end_time": "2024-09-19T03:40:11.088883",
          "exception": false,
          "start_time": "2024-09-19T03:40:11.018477",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "class Conv1D(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv1d(1, 128, kernel_size=3, dilation=2, bias=False),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Conv1d(128, 128, kernel_size=3, dilation=2, bias=False),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.MaxPool1d(2),\n",
        "            nn.GroupNorm(1, 128)\n",
        "        )\n",
        "\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv1d(128, 128, kernel_size=3, dilation=2, bias=False),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Conv1d(128, 128, kernel_size=3, dilation=2, bias=False),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.MaxPool1d(2),\n",
        "            nn.GroupNorm(1, 128)\n",
        "        )\n",
        "\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv1d(128, 128, kernel_size=3, dilation=2, bias=False),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.MaxPool1d(2),\n",
        "            nn.GroupNorm(1, 128)\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.conv2(out)\n",
        "        out = self.conv3(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 374,
      "id": "af7a514a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-19T03:40:11.183138Z",
          "iopub.status.busy": "2024-09-19T03:40:11.182715Z",
          "iopub.status.idle": "2024-09-19T03:40:11.192730Z",
          "shell.execute_reply": "2024-09-19T03:40:11.191212Z"
        },
        "id": "af7a514a",
        "papermill": {
          "duration": 0.059481,
          "end_time": "2024-09-19T03:40:11.195707",
          "exception": false,
          "start_time": "2024-09-19T03:40:11.136226",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "class SENet(nn.Module):\n",
        "    def __init__(self, channels, reduction=16):\n",
        "        super().__init__()\n",
        "\n",
        "        self.se_net = nn.Sequential(\n",
        "            nn.Linear(channels, channels // reduction),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(channels // reduction, channels),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.global_pooling_bridge = nn.AdaptiveAvgPool1d(1)\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.global_pooling_bridge(x)\n",
        "#         print(\"shape input to se net: \", out.shape)\n",
        "        out = self.flatten(out)\n",
        "        out = self.se_net(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 375,
      "id": "a0ab70e1",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-19T03:40:11.284576Z",
          "iopub.status.busy": "2024-09-19T03:40:11.283370Z",
          "iopub.status.idle": "2024-09-19T03:40:11.295009Z",
          "shell.execute_reply": "2024-09-19T03:40:11.293247Z"
        },
        "id": "a0ab70e1",
        "papermill": {
          "duration": 0.059045,
          "end_time": "2024-09-19T03:40:11.297973",
          "exception": false,
          "start_time": "2024-09-19T03:40:11.238928",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "class DaggerNetV2(nn.Module):\n",
        "    def __init__(self, ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.cnn_encoder = Conv1D()\n",
        "\n",
        "        self.se_net = SENet(channels=128)\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        self.global_pooling = nn.AdaptiveAvgPool1d(1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        out = self.cnn_encoder(x)\n",
        "        residual = out\n",
        "\n",
        "#         print(\"CNN out: \", out.shape)\n",
        "\n",
        "        attn_out = self.se_net(out)\n",
        "\n",
        "#         print(\"SE net out: \", attn_out.shape)\n",
        "\n",
        "        attn_out = attn_out.unsqueeze(dim=-1)\n",
        "\n",
        "        out_total = attn_out * residual\n",
        "\n",
        "#         out_total = self.global_pooling(out_total)\n",
        "        out_total = self.flatten(out_total)\n",
        "\n",
        "#         print(\"Output: \", out_total.shape)\n",
        "\n",
        "        return out_total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 376,
      "id": "9bef0b59",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-19T03:40:11.388771Z",
          "iopub.status.busy": "2024-09-19T03:40:11.388306Z",
          "iopub.status.idle": "2024-09-19T03:40:11.405557Z",
          "shell.execute_reply": "2024-09-19T03:40:11.403829Z"
        },
        "id": "9bef0b59",
        "papermill": {
          "duration": 0.067194,
          "end_time": "2024-09-19T03:40:11.409066",
          "exception": false,
          "start_time": "2024-09-19T03:40:11.341872",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "class AudioFeatureExtractor(nn.Module):\n",
        "    def __init__(self, rnn_hidden_size, rnn_num_layers):\n",
        "        super().__init__()\n",
        "\n",
        "        self.rnn_hidden_size = rnn_hidden_size\n",
        "        self.rnn_num_layers = rnn_num_layers\n",
        "\n",
        "\n",
        "        self.zcr_net = DaggerNetV2()\n",
        "        self.rms_net = DaggerNetV2()\n",
        "        self.mfcc_net = DaggerNetV2()\n",
        "        self.mel_net = DaggerNetV2()\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x[\"mfcc\"] = x[\"mfcc\"].unsqueeze(dim=1).float()\n",
        "        x[\"zcr\"] = x[\"zcr\"].unsqueeze(dim=1).float()\n",
        "        x[\"mel\"] = x[\"mel\"].unsqueeze(dim=1).float()\n",
        "        x[\"rms\"] = x[\"rms\"].unsqueeze(dim=1).float()\n",
        "\n",
        "        out_mfcc = self.mfcc_net(x[\"mfcc\"])\n",
        "        out_mel = self.mel_net(x[\"mel\"])\n",
        "\n",
        "        out_zcr = self.zcr_net(x[\"zcr\"])\n",
        "        out_rms = self.rms_net(x[\"rms\"])\n",
        "\n",
        "        combined = torch.cat([out_mfcc, out_zcr, out_rms, out_mel], dim=1)\n",
        "\n",
        "        return combined\n",
        "\n",
        "\n",
        "    def init_hidden(self, batch):\n",
        "        hidden = torch.zeros(self.rnn_num_layers*2, batch, self.rnn_hidden_size).float().to(device)\n",
        "        cell = torch.zeros(self.rnn_num_layers*2, batch, self.rnn_hidden_size).float().to(device)\n",
        "        return hidden, cell"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b20e15f4",
      "metadata": {
        "id": "b20e15f4",
        "papermill": {
          "duration": 0.047282,
          "end_time": "2024-09-19T03:40:11.501355",
          "exception": false,
          "start_time": "2024-09-19T03:40:11.454073",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "<br>\n",
        "<hr>\n",
        "<br>\n",
        "\n",
        "## The Multimodal Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 377,
      "id": "a3b9b6be",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-19T03:40:11.595151Z",
          "iopub.status.busy": "2024-09-19T03:40:11.594718Z",
          "iopub.status.idle": "2024-09-19T03:40:11.613068Z",
          "shell.execute_reply": "2024-09-19T03:40:11.611774Z"
        },
        "id": "a3b9b6be",
        "papermill": {
          "duration": 0.068538,
          "end_time": "2024-09-19T03:40:11.615872",
          "exception": false,
          "start_time": "2024-09-19T03:40:11.547334",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "class MainMultimodal(nn.Module):\n",
        "    def __init__(self, num_classes, trainable=False, fine_tune_limit=2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "#         # define video extractor, cut off FCN layer\n",
        "        self.video_extractor = R2plus1D\n",
        "\n",
        "#         # cut off layer fcn\n",
        "        self.video_extractor.fc = PassThrough()\n",
        "\n",
        "\n",
        "        # define audio extractor\n",
        "        self.audio_extractor = AudioFeatureExtractor(rnn_hidden_size=32, rnn_num_layers=1).to(device)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.LayerNorm(512 + 5632),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512 + 5632, num_classes, bias=True),\n",
        "        )\n",
        "\n",
        "        # init dual gpu usuage\n",
        "        self.video_extractor = nn.DataParallel(self.video_extractor)\n",
        "        self.audio_extractor = nn.DataParallel(self.audio_extractor)\n",
        "\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "            Set the model to trainable false\n",
        "        \"\"\"\n",
        "        if trainable is False:\n",
        "            for param in self.video_extractor.parameters():\n",
        "                param.requires_grad = False\n",
        "        else:\n",
        "\n",
        "            \"\"\"\n",
        "                Train all layers\n",
        "            \"\"\"\n",
        "            if fine_tune_limit == 'all':\n",
        "                for param in self.video_extractor.parameters():\n",
        "                    param.requires_grad = True\n",
        "\n",
        "            else:\n",
        "                \"\"\"\n",
        "                    Set the fine tune limits\n",
        "                \"\"\"\n",
        "                count = 0 # keep track for layer count\n",
        "                length = sum(1 for _ in self.video_extractor.module.children()) # get the length of layers\n",
        "                limit = length - fine_tune_limit # set the limit [if length is 7, then limit = 7-2(default) = 5 ---> if count is = or above this we set to trainable ]\n",
        "\n",
        "\n",
        "                for child in self.video_extractor.module.children():\n",
        "                    if count >= limit:\n",
        "                        for param in child.parameters():\n",
        "                            param.requires_grad = True\n",
        "                    else:\n",
        "                        for param in child.parameters():\n",
        "                            param.requires_grad = False\n",
        "\n",
        "                    count += 1\n",
        "\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "\n",
        "    def forward(self, video, audio):\n",
        "\n",
        "        video = torch.permute(video, (0,2,1,3,4))\n",
        "\n",
        "\n",
        "        video_feature_values = self.video_extractor(video)\n",
        "\n",
        "        audio_feature_values = self.audio_extractor(audio)\n",
        "\n",
        "\n",
        "        del video, audio\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "\n",
        "        combined = torch.cat([video_feature_values, audio_feature_values], dim=1)\n",
        "\n",
        "        out_logits = self.fc(combined)\n",
        "        out_softmax = self.softmax(out_logits)\n",
        "\n",
        "\n",
        "        return out_logits, out_softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 378,
      "id": "7442e60c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-19T03:40:11.708813Z",
          "iopub.status.busy": "2024-09-19T03:40:11.707220Z",
          "iopub.status.idle": "2024-09-19T03:40:11.747733Z",
          "shell.execute_reply": "2024-09-19T03:40:11.745675Z"
        },
        "id": "7442e60c",
        "papermill": {
          "duration": 0.091168,
          "end_time": "2024-09-19T03:40:11.751604",
          "exception": false,
          "start_time": "2024-09-19T03:40:11.660436",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "modelV1 = MainMultimodal(len(idx2class), trainable=True, fine_tune_limit=4).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 379,
      "id": "d94318af",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-19T03:40:11.847512Z",
          "iopub.status.busy": "2024-09-19T03:40:11.847025Z",
          "iopub.status.idle": "2024-09-19T03:40:11.853283Z",
          "shell.execute_reply": "2024-09-19T03:40:11.851823Z"
        },
        "id": "d94318af",
        "papermill": {
          "duration": 0.058734,
          "end_time": "2024-09-19T03:40:11.856635",
          "exception": false,
          "start_time": "2024-09-19T03:40:11.797901",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# next(modelV1.parameters()).is_cuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 380,
      "id": "990a7fb4",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-19T03:40:11.949314Z",
          "iopub.status.busy": "2024-09-19T03:40:11.948779Z",
          "iopub.status.idle": "2024-09-19T03:40:11.959071Z",
          "shell.execute_reply": "2024-09-19T03:40:11.957521Z"
        },
        "id": "990a7fb4",
        "papermill": {
          "duration": 0.059274,
          "end_time": "2024-09-19T03:40:11.962038",
          "exception": false,
          "start_time": "2024-09-19T03:40:11.902764",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "optim = torch.optim.AdamW(params=modelV1.parameters(), lr=hyperparams[\"lr\"], betas=hyperparams[\"adam_betas\"], weight_decay=1e-2)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optim, mode='min', factor=0.1, patience=10, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 381,
      "id": "991eb382",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-19T03:40:12.056591Z",
          "iopub.status.busy": "2024-09-19T03:40:12.056118Z",
          "iopub.status.idle": "2024-09-19T03:40:12.085567Z",
          "shell.execute_reply": "2024-09-19T03:40:12.083987Z"
        },
        "id": "991eb382",
        "papermill": {
          "duration": 0.079878,
          "end_time": "2024-09-19T03:40:12.088860",
          "exception": false,
          "start_time": "2024-09-19T03:40:12.008982",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "def train_step(model: torch.nn.Module, dataloader, optimizer, loss_fn, accuracy_fn=None, save_memory=False):\n",
        "    train_loss = 0.0\n",
        "    train_acc = 0.0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    try:\n",
        "        wandb.watch(model, criterion=loss_fn, log=\"all\", log_freq=10)\n",
        "    except:\n",
        "        print(\"NOT LOGGING TO WANDB\")\n",
        "\n",
        "    for batch, (videos, audios, labels, video_paths, audio_paths) in enumerate(dataloader):\n",
        "        labels = labels.type(torch.LongTensor)\n",
        "        videos, labels = videos.to(device), labels.to(device)\n",
        "\n",
        "        y_logits, y_softmax = model(videos, audios)\n",
        "        y_logits, y_softmax = y_logits.to(device), y_softmax.to(device)\n",
        "\n",
        "        # print(y_logits.shape)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        preds = y_softmax.argmax(dim=1).to(device)\n",
        "        videos = videos.detach().cpu()\n",
        "        # audios = audios.detach().cpu()\n",
        "        del videos, audios\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "\n",
        "        # print(labels.shape, preds.shape)\n",
        "\n",
        "        loss = loss_fn(y_logits, labels)\n",
        "        acc = accuracy_fn(preds, labels, num_classes=len(idx2class))\n",
        "        train_loss += loss.item()\n",
        "        train_acc += acc\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        if batch == 0 or batch % 1000 == 0 or batch == len(dataloader) - 1:\n",
        "            sample = random.randint(1, y_logits.shape[0])-1\n",
        "            print(f\"Batch: #{batch} | Train Loss: {loss} | Train Accuracy: {acc}\")\n",
        "            show_example(video_paths[sample], audio_paths[sample], preds[sample].detach().cpu().item(), labels[sample].detach().cpu().item(), save_memory)\n",
        "\n",
        "        del labels\n",
        "        del video_paths\n",
        "        del audio_paths\n",
        "        preds = preds.detach().cpu()\n",
        "        del preds\n",
        "        y_logits = y_logits.detach().cpu()\n",
        "        del y_logits\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "\n",
        "    train_loss /= len(dataloader)\n",
        "    train_acc /= len(dataloader)\n",
        "    print(f\"Total Train loss: {train_loss} | Total Train accuracy: {train_acc}\")\n",
        "    return train_loss, train_acc\n",
        "\n",
        "\n",
        "def eval_step(model: torch.nn.Module, dataloader, loss_fn, accuracy_fn=None, save_memory=False, confusion_matrix=False):\n",
        "    eval_loss = 0.0\n",
        "    eval_acc = 0.0\n",
        "\n",
        "    y_true = []\n",
        "    y_preds = []\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch, (videos, audios, labels, video_paths, audio_paths) in enumerate(dataloader):\n",
        "            labels = labels.type(torch.LongTensor)\n",
        "            videos, labels = videos.to(device), labels.to(device)\n",
        "\n",
        "            y_logits, y_softmax = model(videos, audios)\n",
        "            y_logits, y_softmax = y_logits.to(device), y_softmax.to(device)\n",
        "\n",
        "            preds = y_softmax.argmax(dim=1).to(device)\n",
        "\n",
        "            if confusion_matrix:\n",
        "                y_preds.extend(preds.detach().cpu().numpy())\n",
        "                y_true.extend(labels.detach().cpu().numpy())\n",
        "\n",
        "            videos = videos.detach().cpu()\n",
        "            # audios = audios.detach().cpu()\n",
        "            del videos, audios\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "\n",
        "            loss = loss_fn(y_logits, labels)\n",
        "            acc = accuracy_fn(preds, labels, num_classes=len(idx2class))\n",
        "            eval_loss += loss.item()\n",
        "            eval_acc += acc\n",
        "\n",
        "\n",
        "            if batch == 0 or batch % 1000 == 0 or batch == len(dataloader) - 1:\n",
        "                sample = random.randint(1, y_logits.shape[0])-1\n",
        "                print(f\"Batch: #{batch} | Eval. Loss: {loss} | Eval. Accuracy: {acc}\")\n",
        "                show_example(video_paths[sample], audio_paths[sample], preds[sample].detach().cpu().item(), labels[sample].detach().cpu().item(), save_memory)\n",
        "\n",
        "            del labels\n",
        "            del video_paths\n",
        "            del audio_paths\n",
        "            preds = preds.detach().cpu()\n",
        "            del preds\n",
        "            y_logits = y_logits.detach().cpu()\n",
        "            del y_logits\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "\n",
        "        eval_loss /= len(dataloader)\n",
        "        eval_acc /= len(dataloader)\n",
        "\n",
        "    print(f\"Total Eval. Loss: {eval_loss} | Total Eval. Accuracy: {eval_acc}\")\n",
        "\n",
        "    if confusion_matrix:\n",
        "        return eval_loss, eval_acc, y_true, y_preds\n",
        "    else:\n",
        "        return eval_loss, eval_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 382,
      "id": "93d30e6c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-19T03:40:12.178335Z",
          "iopub.status.busy": "2024-09-19T03:40:12.177749Z",
          "iopub.status.idle": "2024-09-19T03:40:12.189219Z",
          "shell.execute_reply": "2024-09-19T03:40:12.187866Z"
        },
        "id": "93d30e6c",
        "papermill": {
          "duration": 0.060301,
          "end_time": "2024-09-19T03:40:12.192069",
          "exception": false,
          "start_time": "2024-09-19T03:40:12.131768",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "from tqdm.autonotebook import tqdm\n",
        "import time\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 383,
      "id": "41a5d170",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-19T03:40:12.283650Z",
          "iopub.status.busy": "2024-09-19T03:40:12.283181Z",
          "iopub.status.idle": "2024-09-19T03:40:12.290091Z",
          "shell.execute_reply": "2024-09-19T03:40:12.288452Z"
        },
        "id": "41a5d170",
        "papermill": {
          "duration": 0.057178,
          "end_time": "2024-09-19T03:40:12.293150",
          "exception": false,
          "start_time": "2024-09-19T03:40:12.235972",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "epochs = []\n",
        "train_loss_history = []\n",
        "eval_loss_history = []\n",
        "\n",
        "train_accuracy_history = []\n",
        "eval_accuracy_history = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 384,
      "id": "0de0286c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-19T03:40:12.386322Z",
          "iopub.status.busy": "2024-09-19T03:40:12.385850Z",
          "iopub.status.idle": "2024-09-19T03:40:12.392414Z",
          "shell.execute_reply": "2024-09-19T03:40:12.390730Z"
        },
        "id": "0de0286c",
        "papermill": {
          "duration": 0.058454,
          "end_time": "2024-09-19T03:40:12.395757",
          "exception": false,
          "start_time": "2024-09-19T03:40:12.337303",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "best_params = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 385,
      "id": "b9b8e662",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-19T03:40:12.485105Z",
          "iopub.status.busy": "2024-09-19T03:40:12.483592Z",
          "iopub.status.idle": "2024-09-19T03:40:12.495743Z",
          "shell.execute_reply": "2024-09-19T03:40:12.494316Z"
        },
        "papermill": {
          "duration": 0.060145,
          "end_time": "2024-09-19T03:40:12.499032",
          "exception": false,
          "start_time": "2024-09-19T03:40:12.438887",
          "status": "completed"
        },
        "tags": [],
        "id": "b9b8e662"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(state, filename=\"checkpoint.pth.tar\"):\n",
        "    torch.save(state, filename)\n",
        "\n",
        "def load_checkpoint(model, optimizer, filename=\"checkpoint.pth.tar\"):\n",
        "    checkpoint = torch.load(filename)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    start_epoch = checkpoint['epoch']\n",
        "    best_eval_loss = checkpoint['best_eval_loss']\n",
        "\n",
        "    return model, optimizer, start_epoch, best_eval_loss\n",
        "\n",
        "def load_checkpoint_wandb(model, optimizer, filename, run_path):\n",
        "    with wandb.restore(filename, run_path=run_path) as io:\n",
        "        name = io.name\n",
        "    checkpoint = torch.load(name, map_location=torch.device(device))\n",
        "\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    start_epoch = checkpoint['epoch']\n",
        "    best_eval_loss = checkpoint['best_eval_loss']\n",
        "\n",
        "    return model, optimizer, start_epoch, best_eval_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 386,
      "id": "1c424ee9",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-19T03:40:12.590794Z",
          "iopub.status.busy": "2024-09-19T03:40:12.590271Z",
          "iopub.status.idle": "2024-09-19T03:40:12.596980Z",
          "shell.execute_reply": "2024-09-19T03:40:12.595263Z"
        },
        "papermill": {
          "duration": 0.055538,
          "end_time": "2024-09-19T03:40:12.600054",
          "exception": false,
          "start_time": "2024-09-19T03:40:12.544516",
          "status": "completed"
        },
        "tags": [],
        "id": "1c424ee9"
      },
      "outputs": [],
      "source": [
        "start_epoch = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 387,
      "id": "337d9b3f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-19T03:40:12.688266Z",
          "iopub.status.busy": "2024-09-19T03:40:12.687734Z",
          "iopub.status.idle": "2024-09-19T03:40:12.694647Z",
          "shell.execute_reply": "2024-09-19T03:40:12.693119Z"
        },
        "papermill": {
          "duration": 0.05438,
          "end_time": "2024-09-19T03:40:12.697707",
          "exception": false,
          "start_time": "2024-09-19T03:40:12.643327",
          "status": "completed"
        },
        "tags": [],
        "id": "337d9b3f"
      },
      "outputs": [],
      "source": [
        "best_eval_loss = 1e9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 388,
      "id": "8f4603fe",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-19T03:40:16.125612Z",
          "iopub.status.busy": "2024-09-19T03:40:16.125145Z",
          "iopub.status.idle": "2024-09-19T03:40:16.131950Z",
          "shell.execute_reply": "2024-09-19T03:40:16.130446Z"
        },
        "papermill": {
          "duration": 0.057466,
          "end_time": "2024-09-19T03:40:16.135231",
          "exception": false,
          "start_time": "2024-09-19T03:40:16.077765",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f4603fe",
        "outputId": "a10727e1-295b-43d3-ca68-0b87d302628c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 1000000000.0\n"
          ]
        }
      ],
      "source": [
        "print(start_epoch, best_eval_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 389,
      "id": "51149cb0",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-19T03:40:16.226810Z",
          "iopub.status.busy": "2024-09-19T03:40:16.224925Z",
          "iopub.status.idle": "2024-09-19T03:40:16.234438Z",
          "shell.execute_reply": "2024-09-19T03:40:16.233122Z"
        },
        "id": "51149cb0",
        "papermill": {
          "duration": 0.058323,
          "end_time": "2024-09-19T03:40:16.237465",
          "exception": false,
          "start_time": "2024-09-19T03:40:16.179142",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4d9905ac-5d63-4ce6-8d48-2265ea1769f7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "creating run (0.0s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.3"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250912_184325-iow6dy67</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/hiyadullu05-vit-bhopal-university/multimodal-ser/runs/iow6dy67' target=\"_blank\">cremad-ablation-C</a></strong> to <a href='https://wandb.ai/hiyadullu05-vit-bhopal-university/multimodal-ser' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/hiyadullu05-vit-bhopal-university/multimodal-ser' target=\"_blank\">https://wandb.ai/hiyadullu05-vit-bhopal-university/multimodal-ser</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/hiyadullu05-vit-bhopal-university/multimodal-ser/runs/iow6dy67' target=\"_blank\">https://wandb.ai/hiyadullu05-vit-bhopal-university/multimodal-ser/runs/iow6dy67</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tSave memory mode is on. Set `save_memory=False` to see video-audio examples\n",
            "========================== Starting Epoch: # 0 ==========================\n",
            "Batch: #0 | Train Loss: 1.7799670696258545 | Train Accuracy: 0.25\n",
            "Predicted Label: happy\n",
            "Actual Label: happy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-1150801830.py\", line 20, in <cell line: 0>\n",
            "    train_loss, train_acc = train_step(modelV1, trainloader, optim, loss_fn, multiclass_f1_score, save_memory=save_memory)\n",
            "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-1141561911.py\", line 16, in train_step\n",
            "    y_logits, y_softmax = model(videos, audios)\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1879, in _call_impl\n",
            "    return inner()\n",
            "           ^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1827, in inner\n",
            "    result = forward_call(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-1289070121.py\", line 73, in forward\n",
            "    video_feature_values = self.video_extractor(video)\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/data_parallel.py\", line 174, in forward\n",
            "    return self.module(*inputs, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torchvision/models/video/resnet.py\", line 254, in forward\n",
            "    x = self.layer1(x)\n",
            "        ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\", line 244, in forward\n",
            "    input = module(input)\n",
            "            ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torchvision/models/video/resnet.py\", line 114, in forward\n",
            "    out = self.conv2(out)\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\", line 244, in forward\n",
            "    input = module(input)\n",
            "            ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\", line 244, in forward\n",
            "    input = module(input)\n",
            "            ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\", line 717, in forward\n",
            "    return self._conv_forward(input, self.weight, self.bias)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\", line 712, in _conv_forward\n",
            "    return F.conv3d(\n",
            "           ^^^^^^^^^\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">cremad-ablation-C</strong> at: <a href='https://wandb.ai/hiyadullu05-vit-bhopal-university/multimodal-ser/runs/iow6dy67' target=\"_blank\">https://wandb.ai/hiyadullu05-vit-bhopal-university/multimodal-ser/runs/iow6dy67</a><br> View project at: <a href='https://wandb.ai/hiyadullu05-vit-bhopal-university/multimodal-ser' target=\"_blank\">https://wandb.ai/hiyadullu05-vit-bhopal-university/multimodal-ser</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250912_184325-iow6dy67/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1150801830.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0minference_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelV1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulticlass_f1_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_memory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0meval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelV1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcvloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulticlass_f1_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_memory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1141561911.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(model, dataloader, optimizer, loss_fn, accuracy_fn, save_memory)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mvideos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0my_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_softmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudios\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0my_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_softmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_softmax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1878\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1879\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1880\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1881\u001b[0m             \u001b[0;31m# run always called hooks if they have not already been run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36minner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1825\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1827\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1828\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1829\u001b[0m                 for hook_id, hook in (\n",
            "\u001b[0;32m/tmp/ipython-input-1289070121.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, video, audio)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mvideo_feature_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideo_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0maudio_feature_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DataParallel.forward\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/models/video/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/models/video/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 717\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    710\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m             )\n\u001b[0;32m--> 712\u001b[0;31m         return F.conv3d(\n\u001b[0m\u001b[1;32m    713\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m         )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "save_memory = True\n",
        "\n",
        "with wandb.init(project=\"multimodal-ser\", name='cremad-ablation-C'):\n",
        "\n",
        "    wandb.define_metric(\"epoch\")\n",
        "    wandb.define_metric(\"train/*\", step_metric=\"epoch\")\n",
        "    wandb.define_metric(\"val/*\", step_metric=\"epoch\")\n",
        "\n",
        "    if save_memory:\n",
        "        print(\"\\tSave memory mode is on. Set `save_memory=False` to see video-audio examples\")\n",
        "\n",
        "    start = time.time()\n",
        "    for epoch in range(start_epoch, hyperparams[\"epochs\"]):\n",
        "        print(f\"========================== Starting Epoch: # {epoch} ==========================\")\n",
        "\n",
        "        inference_start = time.time()\n",
        "\n",
        "        train_loss, train_acc = train_step(modelV1, trainloader, optim, loss_fn, multiclass_f1_score, save_memory=save_memory)\n",
        "        eval_loss, eval_acc = eval_step(modelV1, cvloader, loss_fn, multiclass_f1_score, save_memory=save_memory)\n",
        "        scheduler.step(eval_loss)\n",
        "\n",
        "        wandb.log({\"train/loss\": train_loss, \"val/loss\": eval_loss, \"train/f1_acc\": train_acc, \"val/f1_acc\": eval_acc}, step=epoch)\n",
        "\n",
        "        inference_total = time.time() - inference_start\n",
        "        convert_inf = str(datetime.timedelta(seconds=inference_total))\n",
        "\n",
        "\n",
        "        print(f\"Epoch: #{epoch} | Total Train Loss: {train_loss} | Total Eval. Loss: {eval_loss} | Train Acc: {train_acc * 100}% | Eval Acc: {eval_acc * 100}% in {convert_inf}\")\n",
        "\n",
        "\n",
        "        epochs.append(epoch+1)\n",
        "        train_loss_history.append(train_loss)\n",
        "        eval_loss_history.append(eval_loss)\n",
        "        train_accuracy_history.append(train_acc.detach().cpu()*100)\n",
        "        eval_accuracy_history.append(eval_acc.detach().cpu()*100)\n",
        "\n",
        "        if eval_loss < best_eval_loss:\n",
        "            best_eval_loss = eval_loss\n",
        "            # save best checkpoint\n",
        "            save_checkpoint({\n",
        "                'model_state_dict': modelV1.state_dict(),\n",
        "                'optimizer_state_dict': optim.state_dict(),\n",
        "                'epoch': epoch,\n",
        "                'best_eval_loss': best_eval_loss\n",
        "            }, filename=\"best_checkpoint.pth.tar\")\n",
        "            wandb.save('/kaggle/working/best_checkpoint.pth.tar')\n",
        "\n",
        "        # save global checkpoint\n",
        "        save_checkpoint({\n",
        "            'model_state_dict': modelV1.state_dict(),\n",
        "            'optimizer_state_dict': optim.state_dict(),\n",
        "            'epoch': epoch + 1,\n",
        "            'best_eval_loss': eval_loss\n",
        "        }, filename=\"checkpoint.pth.tar\")\n",
        "        wandb.save('/kaggle/working/checkpoint.pth.tar')\n",
        "\n",
        "        # after retry loop\n",
        "        if 'video' not in locals():\n",
        "    # fallback to a dummy tensor if video completely fails\n",
        "           video = torch.zeros((1, 3, 64, 64))  # [frames, channels, H, W]\n",
        "        else:\n",
        "           video = torch.permute(video, (0, 3, 1, 2))\n",
        "\n",
        "\n",
        "        del train_loss, eval_loss, train_acc, eval_acc\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91e14067",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-19T03:40:16.327516Z",
          "iopub.status.busy": "2024-09-19T03:40:16.326958Z",
          "iopub.status.idle": "2024-09-19T03:40:16.575742Z",
          "shell.execute_reply": "2024-09-19T03:40:16.574371Z"
        },
        "id": "91e14067",
        "papermill": {
          "duration": 0.296811,
          "end_time": "2024-09-19T03:40:16.578414",
          "exception": false,
          "start_time": "2024-09-19T03:40:16.281603",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d327563",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-19T03:40:16.864732Z",
          "iopub.status.busy": "2024-09-19T03:40:16.863263Z",
          "iopub.status.idle": "2024-09-19T03:40:16.870809Z",
          "shell.execute_reply": "2024-09-19T03:40:16.869454Z"
        },
        "id": "2d327563",
        "papermill": {
          "duration": 0.05506,
          "end_time": "2024-09-19T03:40:16.873599",
          "exception": false,
          "start_time": "2024-09-19T03:40:16.818539",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# epoch = hyperparams[\"epochs\"]\n",
        "epoch = len(epochs) + 1\n",
        "\n",
        "plt.plot(epochs, train_loss_history, color='dodgerblue', label='Train Loss')\n",
        "plt.plot(epochs, eval_loss_history, color='orange', label='Eval. Loss')\n",
        "\n",
        "\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss Value\")\n",
        "plt.title(f\"Train and Eval. Loss along {epoch} epochs (RAVDESS)\")\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.savefig(\"Loss curves.png\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7371f73",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-19T03:40:16.962185Z",
          "iopub.status.busy": "2024-09-19T03:40:16.960265Z",
          "iopub.status.idle": "2024-09-19T03:40:16.967661Z",
          "shell.execute_reply": "2024-09-19T03:40:16.966066Z"
        },
        "id": "f7371f73",
        "papermill": {
          "duration": 0.054305,
          "end_time": "2024-09-19T03:40:16.970525",
          "exception": false,
          "start_time": "2024-09-19T03:40:16.916220",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "plt.plot(epochs, train_accuracy_history, color='dodgerblue', label='Train Accuracy')\n",
        "plt.plot(epochs, eval_accuracy_history, color='orange', label='Eval. Accuracy')\n",
        "\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"F1 Score Value\")\n",
        "plt.title(f\"Train and Eval. Accuracy along {epoch} epochs (RAVDESS)\")\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "plt.savefig(\"F1-Score curves.png\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7c0a7a8",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-19T03:40:17.255987Z",
          "iopub.status.busy": "2024-09-19T03:40:17.254310Z",
          "iopub.status.idle": "2024-09-19T07:50:26.926961Z",
          "shell.execute_reply": "2024-09-19T07:50:26.924959Z"
        },
        "id": "d7c0a7a8",
        "papermill": {
          "duration": 15009.725399,
          "end_time": "2024-09-19T07:50:26.931930",
          "exception": false,
          "start_time": "2024-09-19T03:40:17.206531",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "modelV1, _, _, _ = load_checkpoint(modelV1, optim, \"./best_checkpoint.pth.tar\")\n",
        "\n",
        "test_loss, test_acc, y_true, y_preds = eval_step(modelV1, testloader, loss_fn, multiclass_f1_score, save_memory=False, confusion_matrix=True)\n",
        "test_acc = test_acc.detach().cpu()\n",
        "\n",
        "print(f\"Test loss: {test_loss}\\tTest Accuracy: {test_acc*100}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f33fbd13",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-19T07:50:27.067741Z",
          "iopub.status.busy": "2024-09-19T07:50:27.066945Z",
          "iopub.status.idle": "2024-09-19T07:50:27.076657Z",
          "shell.execute_reply": "2024-09-19T07:50:27.075432Z"
        },
        "id": "f33fbd13",
        "papermill": {
          "duration": 0.081743,
          "end_time": "2024-09-19T07:50:27.080387",
          "exception": false,
          "start_time": "2024-09-19T07:50:26.998644",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "def format_margins(ax, x=0.05, y=0.05):\n",
        "    xlim = ax.get_xlim()\n",
        "    ylim = ax.get_ylim()\n",
        "\n",
        "    xmargin = (xlim[1]-xlim[0])*x\n",
        "    ymargin = (ylim[1]-ylim[0])*y\n",
        "\n",
        "    ax.set_xlim(xlim[0]-xmargin,xlim[1]+xmargin)\n",
        "    ax.set_ylim(ylim[0]-ymargin,ylim[1]+ymargin)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e7db132",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-19T07:50:27.204201Z",
          "iopub.status.busy": "2024-09-19T07:50:27.203756Z",
          "iopub.status.idle": "2024-09-19T07:50:37.692416Z",
          "shell.execute_reply": "2024-09-19T07:50:37.690611Z"
        },
        "id": "3e7db132",
        "papermill": {
          "duration": 10.555701,
          "end_time": "2024-09-19T07:50:37.696942",
          "exception": false,
          "start_time": "2024-09-19T07:50:27.141241",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "classes = [v for k,v in idx2class.items()]\n",
        "\n",
        "cf_matrix = confusion_matrix(y_true, y_preds)\n",
        "\n",
        "df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1)[:, None], index = [i for i in classes], columns = [i for i in classes])\n",
        "\n",
        "plt.figure(figsize = (12,7))\n",
        "\n",
        "s = sn.heatmap(df_cm, annot=True, cmap='Blues', fmt=\".2f\")\n",
        "\n",
        "plt.xlabel('Predicted Label', fontsize=14, labelpad=20, fontweight='bold')\n",
        "\n",
        "plt.ylabel('True Label', fontsize=14, labelpad=20, fontweight='bold')\n",
        "\n",
        "# format_margins(s, x=0.1)\n",
        "\n",
        "plt.savefig('confusion_matrix_crema.png', dpi=1200)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3baec211",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-19T07:50:38.078240Z",
          "iopub.status.busy": "2024-09-19T07:50:38.077700Z",
          "iopub.status.idle": "2024-09-19T07:50:38.148032Z",
          "shell.execute_reply": "2024-09-19T07:50:38.146407Z"
        },
        "id": "3baec211",
        "papermill": {
          "duration": 0.387833,
          "end_time": "2024-09-19T07:50:38.151323",
          "exception": false,
          "start_time": "2024-09-19T07:50:37.763490",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "report = classification_report(y_true, y_preds, target_names=[v for k,v in idx2class.items()], output_dict=True)\n",
        "\n",
        "df = pd.DataFrame(report).transpose()\n",
        "\n",
        "\n",
        "df = df.round(decimals=4)\n",
        "\n",
        "df.to_csv('classification_report_crema.csv')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75aa649f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-19T07:50:38.280477Z",
          "iopub.status.busy": "2024-09-19T07:50:38.279631Z",
          "iopub.status.idle": "2024-09-19T07:50:38.291664Z",
          "shell.execute_reply": "2024-09-19T07:50:38.289730Z"
        },
        "id": "75aa649f",
        "papermill": {
          "duration": 0.082393,
          "end_time": "2024-09-19T07:50:38.295508",
          "exception": false,
          "start_time": "2024-09-19T07:50:38.213115",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Save stats\n",
        "with open(\"recorded.txt\", \"w\") as f:\n",
        "    f.write(\"R2plus1D & CNN+SE net attempt CREMA\\n\")\n",
        "    for i, line in enumerate(epochs):\n",
        "        f.write(f\"Epoch: {line}: | Train Loss: {train_loss_history[i]} | Train Accuracy: {train_accuracy_history[i]} | Eval Loss: {eval_loss_history[i]} | Eval Accuracy: {eval_accuracy_history[i]}\")\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "    f.write(\"\\n==================================================\\n\")\n",
        "    f.write(f\"On best weights => Test loss: {test_loss}\\tTest Accuracy: {test_acc*100}\")\n",
        "    f.write(\"\\n==================================================\\n\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1a35cc5",
      "metadata": {
        "id": "b1a35cc5",
        "papermill": {
          "duration": 0.061252,
          "end_time": "2024-09-19T07:50:38.418623",
          "exception": false,
          "start_time": "2024-09-19T07:50:38.357371",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## **Citations**\n",
        "\n",
        "1. Livingstone SR, Russo FA (2018) The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English. PLoS ONE 13(5): e0196391. https://doi.org/10.1371/journal.pone.0196391"
      ]
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 3517045,
          "sourceId": 6133877,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30559,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 15130.744939,
      "end_time": "2024-09-19T07:50:41.478915",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2024-09-19T03:38:30.733976",
      "version": "2.4.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}